{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 1. FULL COLUMN LIST ---\n",
      "['index', 'sharefold', 'nonsharefold']\n",
      "\n",
      "--- 2. DATA PREVIEW (First 3 rows) ---\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>sharefold</th>\n",
       "      <th>nonsharefold</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index  sharefold  nonsharefold\n",
       "0      0        2.0           8.0\n",
       "1      1        2.0           8.0\n",
       "2      2        0.0           5.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- 3. DATA SHAPE ---\n",
      "Total Rows: 98415610\n",
      "Total Columns: 3\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Path check\n",
    "path = \"../data/fold.parquet\"\n",
    "\n",
    "try:\n",
    "    # Load the data properly\n",
    "    df = pd.read_parquet(path, engine='pyarrow')\n",
    "    \n",
    "    print(\"--- 1. FULL COLUMN LIST ---\")\n",
    "    print(df.columns.tolist())\n",
    "    \n",
    "    print(\"\\n--- 2. DATA PREVIEW (First 3 rows) ---\")\n",
    "    # This shows the actual content of the columns\n",
    "    display(df.head(3))\n",
    "    \n",
    "    print(\"\\n--- 3. DATA SHAPE ---\")\n",
    "    print(f\"Total Rows: {df.shape[0]}\")\n",
    "    print(f\"Total Columns: {df.shape[1]}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Streaming a portion of the dataset to avoid Memory Error...\n",
      "Success! 'train_sample_100k.parquet' is created.\n",
      "New file size is much smaller and ready for coding!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "path_train = \"../data/train_enc.parquet\"\n",
    "\n",
    "print(\"Streaming a portion of the dataset to avoid Memory Error...\")\n",
    "\n",
    "# 1. Use PyArrow to open the file without loading it into RAM\n",
    "parquet_file = pq.ParquetFile(path_train)\n",
    "\n",
    "# 2. Read only the first 200,000 rows into a table\n",
    "subset_table = parquet_file.read_row_group(0) # Reading the first 'group' of data\n",
    "df_subset = subset_table.to_pandas()\n",
    "\n",
    "# 3. Now take our 100,000 samples from that subset\n",
    "df_sample = df_subset.sample(n=100000, random_state=42)\n",
    "\n",
    "# 4. Save the small version\n",
    "df_sample.to_parquet(\"../data/train_sample_100k.parquet\")\n",
    "\n",
    "print(\"Success! 'train_sample_100k.parquet' is created.\")\n",
    "print(f\"New file size is much smaller and ready for coding!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training shapes: X=(80000, 142), y=(80000, 3)\n",
      "Validation shapes: X=(20000, 142), y=(20000, 3)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# 1. Load new sample file\n",
    "df = pd.read_parquet(\"../data/train_sample_100k.parquet\")\n",
    "\n",
    "# 2. Separate Features (X) and Labels (y)\n",
    "X_cols = [f'enc{i}' for i in range(142)]\n",
    "y_cols = ['bind1', 'bind2', 'bind3']\n",
    "\n",
    "X = df[X_cols].values\n",
    "y = df[y_cols].values\n",
    "\n",
    "# 3. Split into 80% Training and 20% Validation\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f\"Training shapes: X={X_train.shape}, y={y_train.shape}\")\n",
    "print(f\"Validation shapes: X={X_val.shape}, y={y_val.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Protein 1 Binding Rate: 0.1480%\n",
      "Protein 2 Binding Rate: 0.1900%\n",
      "Protein 3 Binding Rate: 0.1280%\n"
     ]
    }
   ],
   "source": [
    "positive_rates = (y.sum(axis=0) / len(y)) * 100\n",
    "for i, rate in enumerate(positive_rates):\n",
    "    print(f\"Protein {i+1} Binding Rate: {rate:.4f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\anaconda3\\envs\\thesis_env\\lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:97: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Training...\n",
      "Epoch 1/3\n",
      "\u001b[1m2500/2500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 3ms/step - AUC: 0.5144 - loss: 0.0400 - val_AUC: 0.5599 - val_loss: 0.0126\n",
      "Epoch 2/3\n",
      "\u001b[1m2500/2500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 4ms/step - AUC: 0.5682 - loss: 0.0115 - val_AUC: 0.5560 - val_loss: 0.0123\n",
      "Epoch 3/3\n",
      "\u001b[1m2500/2500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 4ms/step - AUC: 0.6488 - loss: 0.0110 - val_AUC: 0.6405 - val_loss: 0.0114\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "\n",
    "# 1. Define the model architecture\n",
    "def build_cnn(input_shape):\n",
    "    model = models.Sequential([\n",
    "        # The Embedding layer turns integers into dense vectors of fixed size\n",
    "        layers.Embedding(input_dim=38, output_dim=64, input_length=input_shape),\n",
    "        \n",
    "        # 1D Conv layer to find local chemical patterns\n",
    "        layers.Conv1D(filters=32, kernel_size=3, activation='relu'),\n",
    "        layers.GlobalMaxPooling1D(),\n",
    "        \n",
    "        # Hidden layer for higher-level reasoning\n",
    "        layers.Dense(64, activation='relu'),\n",
    "        layers.Dropout(0.1), # Prevents overfitting\n",
    "        \n",
    "        # Output layer: 3 nodes (one for each protein) with Sigmoid activation\n",
    "        layers.Dense(3, activation='sigmoid') \n",
    "    ])\n",
    "    \n",
    "    model.compile(optimizer='adam', \n",
    "                  loss='binary_crossentropy', \n",
    "                  metrics=['AUC']) # AUC is better than Accuracy for drug discovery\n",
    "    return model\n",
    "\n",
    "# 2. Initialize the model\n",
    "model = build_cnn(input_shape=142)\n",
    "\n",
    "# 3. Train it (start with just 3 epochs to test)\n",
    "print(\"Starting Training...\")\n",
    "history = model.fit(X_train, y_train, \n",
    "                    validation_data=(X_val, y_val), \n",
    "                    epochs=3, \n",
    "                    batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a models folder\n",
    "import os\n",
    "if not os.path.exists('../models'):\n",
    "    os.makedirs('../models')\n",
    "\n",
    "# Save the model\n",
    "model.save('../models/initial_cnn_v1.keras')\n",
    "print(\"Model saved to /models/initial_cnn_v1.keras\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.25 ('thesis_env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.25"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "cd1fc6b71240a2cb2cd6601bfe6e53055c4b6ca0304a07da0ad8d22e97ac852e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
